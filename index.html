<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>IA para LS | Comunicação</title>
  <link rel="stylesheet" href="style.css">
  <link rel="shortcut icon" href="img/logo.jpeg" type="image/x-icon">
</head>
<body>
  <nav class="navbar">
    <div class="logo">
        <a href="https://iefponline.iefp.pt/IEFP/index2.jsp" target="_blank"><img src="img/logo_iefp.jpg" alt=""></a>
    </div>
    <ul class="nav-menu">
      <li><a href="index.html">Comunicação</a></li>
      <li><a href="colaboracao.html">Colaboração</a></li>
      <li><a href="registar.html">Registar</a></li>
      <li><a href="hospedagem.html">Hospedar Site</a></li>
      <li><a href="sobreNos.html">Sobre Nós</a></li>
    </ul>
  </nav>

  <main class="content">
    <div class="text-column">
        <h1>IA para Linguagem de Sinais</h1>
        <p>
          <span class="highlight">No mundo</span>, são <span class="highlight">mais de 430 milhões</span> de <span class="highlight">pessoas surdas e/ou com deficiência auditiva</span>.
          Tais desafios motivaram a <span class="highlight">Lenovo e o CESAR</span> (Centro de Estudos e Sistemas Avançados do Recife), a criarem uma <span class="highlight">tecnologia</span> proprietária <span class="highlight">de IA </span>capaz de <span class="highlight">identificar visualmente</span> e <span class="highlight">contextualizar gestos individuais</span> através de um conjunto de dados de milhares de vídeos de Língua Brasileira de Sinais.
        </p>

        <h2>COMO FUNCIONA?</h2>
        <p>
          A ideia é que a solução permita que <span class="highlight">pessoas com deficiência auditiva façam sinais para a câmera de um dispositivo enquanto um algoritmo realiza a tradução simultânea em texto, em português</span>.
          A tecnologia também pode ser usada para ensinar a linguagem de sinais a pessoas ouvintes, já que é possível usar imagens computacionais para rastrear a precisão dos gestos em relação ao banco de dados personalizado em construção.
        </p>
        <p>
          Assim, em vez de tentar traduzir sinal por sinal, a <span class="highlight">inteligencia artificial e o banco de dados</span> em desenvolvimento estão a aprender a <span class="highlight">reconhecer posições das mãos</span>, extrapolando dados das curvas das mãos e, principalmente, dos pontos de articulação digital dos ossos do sinalizador.
          Uma vez que o algoritmo pode <span class="highlight">reconhecer e processar esses movimentos e gestos</span> rapidamente e com precisão, <span class="highlight">em tempo real</span>, é possível reconhecer o fluxo de uma frase e <span class="highlight">traduzi-la para texto</span>.
        </p>
        <br>
    </div>
    <div class="image-column">
      <iframe width="700" height="500"
      src="https://www.youtube.com/embed/QOs41L3pW9k?si=VF_e07xD4rAh2Wi1" 
      title="Vídeo explicativo sobre a tecnologia de IA para surdos" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" 
      allowfullscreen></iframe>
    </div>
  </main>

  <footer class="footer">
    <p>&copy; 2024 <a href="https://iefponline.iefp.pt/IEFP/centros-emprego-detalhe.do?idcentro=269" target="_blank">IEFP - Alcoitão</a> <br>
      Formadora Inês Freire dos Santos</p>
  </footer>
</body>
</html>